#getting the content of pull request comment pages in jsonfiles
install.packages("jsonlite")
library('jsonlite')
install.packages('httpuv')
library(httpuv)
install.packages('httr')
library('httr')

myapi <- oauth_app("github" "3f2c05f63b3d9cebf87f", secret="03554cba5aa1d4730737ce85cf8d0ccd599dc661")

github_token <- oauth2.0_token(oauth_endpoints("github", myapi)
                               
request1 <- GET("https://github.com/aronlindberg/hub_minr/pull/1",config(token=github_token))
myjson <- content(request1)
myjson2 <- jsonlite::fromJSON(toJSON(myjson))
view(myjson2)
stop_for_status(request1)

#the content should the comment , id, dates & all the commits included, Using this json document we can preprocess the data to the abstract 
# if the code gives some I have check with the syntax of ouath_app again
#there are certain issues in running this with R studio , I will check and update them tommarow












#topic modelling & optimizing no of topics& Lda working In r

library(devtools)
install_github(ropensci/efile)
library(efile)
library(topicmodels)
raw <- searchelife(terms = "*", searchin = "topic_title", boolean = "matches")

abstarcts <- sapply(raw, function(x) elife_doi(x, ret = "abstract")) #for obtaining the abstract of the topics in the request
areas <- sapply(raw, function(x) elife_doi(x, ret = "subject_area")) #for the obtaining the topic_area 
dates <- sapply(raw, function(x) elife_doi(x, ret = "pub_date_date") #for obtaining the comment date

Future plan 
Data preprocessing matrix of the comments

library(elife) # pacakage to be downloaded from github
library(tm)
library(topicmodels)
library(LDAvis)

comments <- read.csv(filepath)
abstarcts <- sapply(raw, function(x) elife_doi(x, ret = "abstract")) #for obtaining the abstract of the topics in the request
# preprocessing the data

dtm.control <- list( tolower = True, #converting to lowercase
removePunctuation = True, #removing punctuation
removenumbers = True,  #numbers deletion
stopwords = stopwords('english') #removing stopwords
stemming = True, # removing suffixes
wordslength = c(5, Inf)
weighting = weightTF)

corp <- Corpus(VectorSource(unlist(abstracts, use.names = False))
dtm <- DocumentTermMatrix(corp, control = dtm.control)
dim(dtm)
dtm <- removeSparseTerms(dtm, 0.99)


creating the corpus
Tokenizing the words
Finding the most frequent bi grams & trigrams for the given corpus





#Different ways for dealing with deal with the login
ctx <- interactive.login(client_id, client_secret,
    scopes = NULL, base_url = "https://github.com",
    api_url = "https://api.github.com", max_etags = 10000)

#with source
ctx <- interactive.login(source(credentials.R, echo =True)


#ADDING A INTERACTIVE LOGIN CREDENTIALS IN R
getLoginDetails <- function(){
  ## Based on code by Barry Rowlingson
  ## http://r.789695.n4.nabble.com/tkentry-that-exits-after-RETURN-tt854721.html#none
  require(tcltk)
  tt <- tktoplevel()
  tkwm.title(tt, "Get login details")
  Name <- tclVar("User ID")
  Password <- tclVar("Password")
  entry.Name <- tkentry(tt,width="20", textvariable=Name)
  entry.Password <- tkentry(tt, width="20", show="*", 
                            textvariable=Password)
  tkgrid(tklabel(tt, text="Please enter your login details."))
  tkgrid(entry.Name)
  tkgrid(entry.Password)
  
  OnOK <- function()
  { 
    tkdestroy(tt) 
  }
  OK.but <-tkbutton(tt,text=" OK ", command=OnOK)
  tkbind(entry.Password, "<Return>", OnOK)
  tkgrid(OK.but)
  tkfocus(tt)
  tkwait.window(tt)
  
  invisible(c(loginID=tclvalue(Name), password=tclvalue(Password)))
}
credentials <- getLoginDetails()
## Do what needs to be done
## Delete credentials
rm(credentials)
